{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2079fa-b09a-4344-a761-39a3ee96b505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ra_tanvi/.conda/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 164923\n",
      "Validation size: 5183\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"google/code_x_glue_ct_code_to_text\", \"java\")\n",
    "\n",
    "# Reduce size for faster iteration\n",
    "import random\n",
    "train_data = dataset[\"train\"].select(random.sample(range(len(dataset[\"train\"])), k=int(1.0 * len(dataset[\"train\"]))))\n",
    "val_data = dataset[\"validation\"].select(random.sample(range(len(dataset[\"validation\"])), k=int(1.0 * len(dataset[\"validation\"]))))\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Validation size:\", len(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c7e983-33a7-4b98-863f-ddb1106f8818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>', 'sep_token': '<sep>', 'bos_token': '<s>', 'eos_token': '</s>'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0bf3bef-bffa-4e2f-b899-7cf9ffa3d117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████| 164923/164923 [02:43<00:00, 1010.17 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████| 5183/5183 [00:05<00:00, 1015.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    code = example[\"code\"]\n",
    "    docstring = example[\"docstring\"]\n",
    "    full_text = f\"<s> {code} </s> <sep> {docstring}\"\n",
    "\n",
    "    # Tokenize the full string\n",
    "    tokens = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Copy labels from input_ids\n",
    "    labels = tokens[\"input_ids\"][:]\n",
    "\n",
    "    # Mask out the code portion\n",
    "    sep_id = tokenizer.convert_tokens_to_ids(\"<sep>\")\n",
    "    try:\n",
    "        sep_index = labels.index(sep_id)\n",
    "    except ValueError:\n",
    "        sep_index = 0  # fallback: mask entire sequence\n",
    "\n",
    "    labels[:sep_index + 1] = [-100] * (sep_index + 1)\n",
    "    tokens[\"labels\"] = labels\n",
    "\n",
    "    return tokens\n",
    "train_data = train_data.map(preprocess, remove_columns=train_data.column_names)\n",
    "val_data = val_data.map(preprocess, remove_columns=val_data.column_names)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a6dc54-e6c2-4d26-b60d-55717bb96581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c83f512-99eb-4cf5-be4a-5e9a3380c5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 41\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Create the Trainer\u001b[39;00m\n\u001b[1;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     32\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     33\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     42\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     43\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mipc_collect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import os\n",
    "\n",
    "# Load and resize the pretrained model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize for new tokens\n",
    "\n",
    "# Optional: Enable progress bar\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable WandB if accidentally enabled\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-docstring\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,                       # 5 epochs\n",
    "    eval_strategy=\"epoch\",                    # evaluate every epoch\n",
    "    save_strategy=\"epoch\",                    # save model every epoch\n",
    "    logging_dir=\"./logs\",                     # logs directory\n",
    "    logging_steps=1,                          # log every step for progress bar\n",
    "    disable_tqdm=False,                       # ✅ ensure tqdm is enabled\n",
    "    report_to=\"none\",                         # no external logging (e.g. WandB)\n",
    "    logging_first_step=True\n",
    ")\n",
    "\n",
    "# Data collator to handle padding and masking\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# ✅ Start training with progress bar\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "811686b8-acb2-482b-921d-059957152f43",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save model and tokenizer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./gpt2-docstring-model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./gpt2-docstring-model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3564\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3559\u001b[39m     gc.collect()\n\u001b[32m   3561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   3562\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   3563\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3564\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3565\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3566\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/safetensors/torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata=metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/safetensors/torch.py:500\u001b[39m, in \u001b[36m_flatten\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    493\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    497\u001b[39m     k: {\n\u001b[32m    498\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v.dtype).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m],\n\u001b[32m    499\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m: v.shape,\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    501\u001b[39m     }\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors.items()\n\u001b[32m    503\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/safetensors/torch.py:422\u001b[39m, in \u001b[36m_tobytes\u001b[39m\u001b[34m(tensor, name)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    415\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` which is not allowed. It either means you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms recommended to save\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m pack it before saving.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m     )\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensor.device.type != \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./gpt2-docstring-model\")\n",
    "tokenizer.save_pretrained(\"./gpt2-docstring-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e895bb-a40c-4ad6-887e-3f08a08c2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-docstring-model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-docstring-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d2651a5-5151-4ef7-a13c-d7378fd9e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docstring_few_shot(test_code, max_length=200):\n",
    "    few_shot_prompt = \"\"\"\n",
    "<s> public int add(int a, int b) { return a + b; } </s> <sep> Adds two integers and returns the sum.\n",
    "\n",
    "<s> public int multiply(int a, int b) { return a * b; } </s> <sep> Multiplies two integers and returns the product.\n",
    "\n",
    "<s> public boolean isEven(int num) { return num % 2 == 0; } </s> <sep> Checks if a number is even.\n",
    "\n",
    "<s> public String greet(String name) { return \"Hello \" + name; } </s> <sep> Greets the user by name.\n",
    "\"\"\"\n",
    "\n",
    "    # Append the new example\n",
    "    prompt =  f\"<s> {test_code} </s> <sep>\"\n",
    "\n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_len = input_ids.shape[1]\n",
    "\n",
    "    # Generate continuation from after the prompt\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_len + 50,  # buffer for generation\n",
    "        num_beams=9,\n",
    "        no_repeat_ngram_size=4,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Only decode the newly generated tokens (after prompt)\n",
    "    generated_ids = output_ids[0][input_len:]  # exclude prompt\n",
    "    result = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18660e7-1f33-409e-ad23-f4b611d1d988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated docstring:\n",
      " Multiply two integers.\n"
     ]
    }
   ],
   "source": [
    "# java_code = \"public boolean isEqual(int a, int b) { return a==b; }\"\n",
    "java_code = \"public int multiply(int a, int b) { return a * b; }\"\n",
    "docstring = generate_docstring_few_shot(java_code)\n",
    "print(\"Generated docstring:\\n\", docstring.split(\"\\n\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2faf1b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated docstring:\n",
      " Determines if the given string is a Palindrome.\n",
      "\n",
      "@param s the string to test.\n",
      "@return true if the string is Palindrome, false otherwise.\n",
      "@since 1.0.0\n",
      "@see #\n"
     ]
    }
   ],
   "source": [
    "# java_code = \"public boolean isEqual(int a, int b) { return a==b; }\"\n",
    "java_code = \"boolean isPalindrome(String s) { return s.equals(new StringBuilder(s).reverse().toString()); }\"\n",
    "docstring = generate_docstring_few_shot(java_code)\n",
    "print(\"Generated docstring:\\n\", docstring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a082afb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated docstring:\n",
      " Removes all whitespace from the input string.\n"
     ]
    }
   ],
   "source": [
    "# java_code = \"public boolean isEqual(int a, int b) { return a==b; }\"\n",
    "java_code = \"public String cleanAndLower(String input) { return input.trim().toLowerCase().replaceAll('[^a-z0-9 ]', ''); }\"\n",
    "docstring = generate_docstring_few_shot(java_code)\n",
    "print(\"Generated docstring:\\n\", docstring.split(\"\\n\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27683a-67f4-411c-939b-7693217ba8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb81c33f-8434-45df-becd-e55c57cdf3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 164923/164923 [00:01<00:00, 139412.72 examples/s]\n",
      "Generating validation split: 100%|██████████| 5183/5183 [00:00<00:00, 93917.07 examples/s]\n",
      "Generating test split: 100%|██████████| 10955/10955 [00:00<00:00, 109530.52 examples/s]\n",
      "Map: 100%|██████████| 5183/5183 [00:12<00:00, 425.51 examples/s]\n",
      "Generating summaries in batch:   0%|          | 0/324 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating summaries in batch:   0%|          | 0/324 [08:56<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 6.14 GB",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(model.device)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# generate up to 64 tokens of summary\u001b[39;49;00m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\n\u001b[32m     58\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m decoded = tokenizer.batch_decode(outputs, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     62\u001b[39m generated_summaries.extend(decoded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2484\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n\u001b[32m   2483\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2484\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2485\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2489\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2490\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2493\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2494\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2495\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2496\u001b[39m         batch_size=batch_size,\n\u001b[32m   2497\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2503\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2504\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/transformers/generation/utils.py:3904\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3901\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3902\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m-> \u001b[39m\u001b[32m3904\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3906\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3907\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3908\u001b[39m     model_outputs,\n\u001b[32m   3909\u001b[39m     model_kwargs,\n\u001b[32m   3910\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3911\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1084\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1081\u001b[39m     torch.cuda.set_device(\u001b[38;5;28mself\u001b[39m.transformer.first_device)\n\u001b[32m   1082\u001b[39m     hidden_states = hidden_states.to(\u001b[38;5;28mself\u001b[39m.lm_head.weight.device)\n\u001b[32m-> \u001b[39m\u001b[32m1084\u001b[39m lm_logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1088\u001b[39m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/proj_doc/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Invalid buffer size: 6.14 GB"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configs\n",
    "model_dir = \"/gpt2-docstring/checkpoint-123693/\"\n",
    "save_path = \"codet5_val_predictions_2.csv\"\n",
    "save_every = 50  # save every N batches\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-docstring-model\").to(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-docstring-model\")\n",
    "model.eval()\n",
    "\n",
    "# Load and tokenize data\n",
    "dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"java\")\n",
    "val_data = dataset[\"validation\"]\n",
    "gold_summaries = val_data[\"docstring\"]\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"code\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "val_tokenized = val_data.map(tokenize_fn, batched=True)\n",
    "val_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "val_loader = DataLoader(val_tokenized, batch_size=16)\n",
    "\n",
    "# Batch inference with intermediate saving\n",
    "generated_summaries = []\n",
    "batch_size = 16\n",
    "start_batch = 0\n",
    "\n",
    "# Resume logic (optional)\n",
    "if os.path.exists(save_path):\n",
    "    df_existing = pd.read_csv(save_path)\n",
    "    generated_summaries = df_existing[\"predicted_summary\"].tolist()\n",
    "    start_batch = len(generated_summaries) // batch_size\n",
    "    print(f\"⏩ Resuming from batch {start_batch} (already {len(generated_summaries)} predictions)\")\n",
    "\n",
    "# Inference loop\n",
    "for i, batch in enumerate(tqdm(val_loader, desc=\"Generating summaries in batch\")):\n",
    "    if i < start_batch:\n",
    "        continue  # skip already done batches\n",
    "\n",
    "    input_ids = batch[\"input_ids\"].to(model.device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=64,   # generate up to 64 tokens of summary\n",
    "        num_beams=4\n",
    "    )\n",
    "\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    generated_summaries.extend(decoded)\n",
    "\n",
    "    # Save every N batches\n",
    "    if (i + 1) % save_every == 0 or (i + 1) == len(val_loader):\n",
    "        print(f\"💾 Saving progress at batch {i + 1}\")\n",
    "        df = pd.DataFrame({\n",
    "            \"gold_summary\": gold_summaries[:len(generated_summaries)],\n",
    "            \"predicted_summary\": generated_summaries\n",
    "        })\n",
    "        df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ead4583-c72a-459a-b633-951bf4b19b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Filtered validation size: 4336 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████| 4336/4336 [00:02<00:00, 1783.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Resuming from batch 323 (already 5183 predictions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries in batch: 100%|█████████████████████████████████████████████████████████████| 271/271 [00:00<00:00, 3135.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configs\n",
    "model_dir = \"/gpt2-docstring/checkpoint-123693/\"\n",
    "save_path = \"codet5_val_predictions.csv\"\n",
    "save_every = 50  # save every N batches\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-docstring-model\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-docstring-model\")\n",
    "model.eval()\n",
    "\n",
    "# Load and tokenize data\n",
    "dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"java\")\n",
    "val_data = dataset[\"validation\"]\n",
    "gold_summaries = val_data[\"docstring\"]\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"code\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Filter out long code snippets\n",
    "def filter_by_token_length(example, tokenizer=tokenizer, max_input_tokens=384):\n",
    "    tokens = tokenizer(example[\"code\"], truncation=False)[\"input_ids\"]\n",
    "    return len(tokens) <= max_input_tokens\n",
    "\n",
    "# Apply filtering\n",
    "val_data = val_data.filter(lambda x: filter_by_token_length(x), batched=False)\n",
    "print(f\"🧹 Filtered validation size: {len(val_data)} examples\")\n",
    "\n",
    "val_tokenized = val_data.map(tokenize_fn, batched=True)\n",
    "val_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "val_loader = DataLoader(val_tokenized, batch_size=16)\n",
    "\n",
    "# Batch inference with intermediate saving\n",
    "generated_summaries = []\n",
    "batch_size = 16\n",
    "start_batch = 0\n",
    "\n",
    "# Resume logic (optional)\n",
    "if os.path.exists(save_path):\n",
    "    df_existing = pd.read_csv(save_path)\n",
    "    generated_summaries = df_existing[\"predicted_summary\"].tolist()\n",
    "    start_batch = len(generated_summaries) // batch_size\n",
    "    print(f\"⏩ Resuming from batch {start_batch} (already {len(generated_summaries)} predictions)\")\n",
    "\n",
    "# Inference loop\n",
    "for i, batch in enumerate(tqdm(val_loader, desc=\"Generating summaries in batch\")):\n",
    "    if i < start_batch:\n",
    "        continue  # skip already done batches\n",
    "\n",
    "    input_ids = batch[\"input_ids\"].to(model.device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128, num_beams=4)\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    generated_summaries.extend(decoded)\n",
    "\n",
    "    # Save every N batches\n",
    "    if (i + 1) % save_every == 0 or (i + 1) == len(val_loader):\n",
    "        print(f\"💾 Saving progress at batch {i + 1}\")\n",
    "        df = pd.DataFrame({\n",
    "            \"gold_summary\": gold_summaries[:len(generated_summaries)],\n",
    "            \"predicted_summary\": generated_summaries\n",
    "        })\n",
    "        df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf7d28b-b714-4728-94d5-9da04f09dffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(83936) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in ./venv/lib/python3.12/site-packages (from bert_score) (2.6.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in ./venv/lib/python3.12/site-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in ./venv/lib/python3.12/site-packages (from bert_score) (4.51.3)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from bert_score) (2.2.5)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in ./venv/lib/python3.12/site-packages (from bert_score) (4.67.1)\n",
      "Collecting matplotlib (from bert_score)\n",
      "  Using cached matplotlib-3.10.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.12/site-packages (from bert_score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (4.13.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (79.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./venv/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->bert_score)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->bert_score)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->bert_score)\n",
      "  Using cached fonttools-4.57.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->bert_score)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib->bert_score)\n",
      "  Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->bert_score)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->bert_score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->bert_score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->bert_score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Using cached matplotlib-3.10.1-cp312-cp312-macosx_11_0_arm64.whl (8.0 MB)\n",
      "Downloading contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.5/255.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.57.0-cp312-cp312-macosx_10_13_universal2.whl (2.8 MB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, bert_score\n",
      "Successfully installed bert_score-0.3.13 contourpy-1.3.2 cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.2.1 pyparsing-3.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25990e57-38ff-4923-94e8-1a013e17cf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 ROUGE Scores:\n",
      "rouge1: 0.3065\n",
      "rouge2: 0.1492\n",
      "rougeL: 0.2683\n",
      "rougeLsum: 0.2928\n",
      "\n",
      "✅ Exact Match Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load predictions CSV\n",
    "csv_path = \"codet5_val_predictions_cleaned_2.csv\"  # update if needed\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract predictions and references\n",
    "predictions = df[\"predicted_summary\"].astype(str).tolist()\n",
    "references = df[\"gold_summary\"].astype(str).tolist()\n",
    "\n",
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute ROUGE\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "print(\"📊 ROUGE Scores:\")\n",
    "for k, v in rouge_scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Load metric and compute\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "result = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "\n",
    "# Compute Exact Match\n",
    "exact_matches = sum([p.strip() == r.strip() for p, r in zip(predictions, references)])\n",
    "exact_match_accuracy = exact_matches / len(references)\n",
    "print(f\"\\n✅ Exact Match Accuracy: {exact_match_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a0695ac-6624-4b12-8bcb-53f2fdb10dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 ROUGE Scores:\n",
      "rouge1: 0.3065\n",
      "rouge2: 0.1492\n",
      "rougeL: 0.2683\n",
      "rougeLsum: 0.2928\n",
      "\n",
      "📘 BLEU Score: 0.1318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 BERTScore F1 (avg): 0.8429\n",
      "\n",
      "✅ Exact Match Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Load predictions CSV\n",
    "df = pd.read_csv(\"codet5_val_predictions_cleaned_2.csv\")\n",
    "\n",
    "# Extract predictions and references\n",
    "predictions = df[\"predicted_summary\"].astype(str).tolist()\n",
    "references = df[\"gold_summary\"].astype(str).tolist()\n",
    "\n",
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# ROUGE\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "print(\"📊 ROUGE Scores:\")\n",
    "for k, v in rouge_scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# BLEU\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"\\n📘 BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "\n",
    "# BERTScore\n",
    "bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"en\", device=\"cuda\")\n",
    "print(f\"\\n🧠 BERTScore F1 (avg): {np.mean(bert_score['f1']):.4f}\")\n",
    "\n",
    "# Function to compute average token repetition per summary\n",
    "def avg_token_repetition(predictions):\n",
    "    rep_counts = []\n",
    "    for text in predictions:\n",
    "        tokens = text.strip().split()\n",
    "        counts = Counter(tokens)\n",
    "        repeated_tokens = sum(v for v in counts.values() if v > 1)\n",
    "        rep_counts.append(repeated_tokens / max(1, len(tokens)))\n",
    "    return np.mean(rep_counts)\n",
    "repetition = avg_token_repetition(predictions)\n",
    "\n",
    "\n",
    "# Exact Match\n",
    "exact_matches = sum(p.strip() == r.strip() for p, r in zip(predictions, references))\n",
    "exact_match_accuracy = exact_matches / len(references)\n",
    "print(f\"\\n✅ Exact Match Accuracy: {exact_match_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97ad1b05-f930-48f4-b47f-efdf33acbd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>BERTScore</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>Avg Token Repetition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3065</td>\n",
       "      <td>0.1492</td>\n",
       "      <td>0.2683</td>\n",
       "      <td>0.1318</td>\n",
       "      <td>0.8429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROUGE-1  ROUGE-2  ROUGE-L    BLEU  BERTScore  Exact Match  \\\n",
       "0   0.3065   0.1492   0.2683  0.1318     0.8429          0.0   \n",
       "\n",
       "   Avg Token Repetition  \n",
       "0                0.5025  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_results = []  # Initialize before appending\n",
    "\n",
    "all_results.append({\n",
    "        # \"Version\": name,\n",
    "        \"ROUGE-1\": round(rouge_scores[\"rouge1\"], 4),\n",
    "        \"ROUGE-2\": round(rouge_scores[\"rouge2\"], 4),\n",
    "        \"ROUGE-L\": round(rouge_scores[\"rougeL\"], 4),\n",
    "        \"BLEU\": round(bleu_score[\"bleu\"], 4),\n",
    "        \"BERTScore\": round(np.mean(bert_score[\"f1\"]), 4),\n",
    "        \"Exact Match\": round(exact_match_accuracy, 4),\n",
    "        \"Avg Token Repetition\": round(repetition, 4)\n",
    "    })\n",
    "\n",
    "# Display comparison table\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values(\"ROUGE-L\", ascending=False)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eca1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cb7c130-76d6-4205-a166-c248dc5cfefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_pipeline.py\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Dict, Optional\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "class CodeSummaryGenerator:\n",
    "    def __init__(self, model_path: str, decoding_config: Dict,device):\n",
    "        self.model_path = model_path\n",
    "        self.decoding_config = decoding_config\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "        self.model =GPT2LMHeadModel.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.eval()\n",
    "        \n",
    "    def preprocess_dataset(self, split: str = \"validation\", max_input_length: int = 512):\n",
    "        dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"java\")[split]\n",
    "\n",
    "        def tokenize_fn(example):\n",
    "            return self.tokenizer(\n",
    "                example[\"code\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_input_length,\n",
    "            )\n",
    "\n",
    "        tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "        tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "        return tokenized, dataset[\"docstring\"]\n",
    "\n",
    "    def generate_summaries(self,\n",
    "                            tokenized_data,\n",
    "                            references: List[str],\n",
    "                            save_path: str,\n",
    "                            batch_size: int = 16,\n",
    "                            save_every: int = 50):\n",
    "\n",
    "        val_loader = DataLoader(tokenized_data, batch_size=batch_size)\n",
    "        generated_summaries = []\n",
    "        start_batch = 0\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            df_existing = pd.read_csv(save_path)\n",
    "            generated_summaries = df_existing[\"predicted_summary\"].astype(str).tolist()\n",
    "            start_batch = len(generated_summaries) // batch_size\n",
    "            print(f\"⏩ Resuming from batch {start_batch} (already {len(generated_summaries)} predictions)\")\n",
    "\n",
    "        for i, batch in enumerate(tqdm(val_loader, desc=\"Generating Summaries\")):\n",
    "            if i < start_batch:\n",
    "                continue\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    **self.decoding_config\n",
    "                )\n",
    "\n",
    "            decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            generated_summaries.extend(decoded)\n",
    "\n",
    "            if (i + 1) % save_every == 0 or (i + 1) == len(val_loader):\n",
    "                print(f\"💾 Saving at batch {i + 1}\")\n",
    "                df = pd.DataFrame({\n",
    "                    \"gold_summary\": references[:len(generated_summaries)],\n",
    "                    \"predicted_summary\": generated_summaries\n",
    "                })\n",
    "                df.to_csv(save_path, index=False)\n",
    "\n",
    "    def generate_single(self, code_snippet: str):\n",
    "        inputs = self.tokenizer(\n",
    "            code_snippet,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(**inputs, **self.decoding_config)\n",
    "\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example usage in a script or notebook:\n",
    "# # if _name_ == \"_main_\":\n",
    "# model_path = GPT2LMHeadModel.from_pretrained(\"./gpt2-docstring-model\")\n",
    "# # tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-docstring-model\")\n",
    "\n",
    "# # Define all 4 decoding strategies\n",
    "# decoding_configs = {\n",
    "#     \"baseline_beam\": {\n",
    "#         \"max_new_tokens\": 64,\n",
    "#         \"num_beams\": 9,\n",
    "#         \"early_stopping\": True\n",
    "#     },\n",
    "#     \"beam_repetition\": {\n",
    "#         \"max_new_tokens\": 64,\n",
    "#         \"num_beams\": 4,\n",
    "#         \"early_stopping\": True,\n",
    "#         \"repetition_penalty\": 1.3\n",
    "#     },\n",
    "#     \"sampling_topk\": {\n",
    "#         \"max_new_tokens\": 64,\n",
    "#         \"do_sample\": True,\n",
    "#         \"top_k\": 50,\n",
    "#         \"temperature\": 0.7,\n",
    "#         \"early_stopping\": True,\n",
    "#         \"repetition_penalty\": 1.2\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Choose best config for final test/validation inference\n",
    "chosen_config = decoding_configs[\"sampling_topk\"]  # Based on best val results\n",
    "save_path = \"codet5_test_sampling_output_sampling.csv\"\n",
    "chosen_config = decoding_configs[\"baseline_beam\"]  # Based on best val results\n",
    "save_path = \"codet5_test_sampling_output_beam.csv\"\n",
    "chosen_config = decoding_configs[\"beam_repetition\"]  # Based on best val results\n",
    "save_path = \"codet5_test_sampling_output_beam_repeat.csv\"\n",
    "\n",
    "generator = CodeSummaryGenerator(model_path, chosen_config)\n",
    "test_tokenized, references = generator.preprocess_dataset(\"test\")\n",
    "generator.generate_summaries(test_tokenized, references, save_path)\n",
    "\n",
    "# Optional: generate on validation set with another config if needed\n",
    "# val_generator = CodeSummaryGenerator(model_path, decoding_configs[\"beam_repetition\"])\n",
    "# val_tokenized, val_refs = val_generator.preprocess_dataset(\"validation\")\n",
    "# val_generator.generate_summaries(val_tokenized, val_refs, \"/content/drive/MyDrive/codet5_val_beam_repetition.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cda093d0-ff81-478e-be4b-665fc2650797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████| 10955/10955 [00:07<00:00, 1386.95 examples/s]\n",
      "Generating Summaries:   0%|                                                                                  | 0/685 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   0%|                                                                          | 1/685 [00:02<28:23,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   0%|▏                                                                         | 2/685 [00:04<28:22,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   0%|▎                                                                         | 3/685 [00:07<28:21,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   1%|▍                                                                         | 4/685 [00:09<28:21,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   1%|▌                                                                         | 5/685 [00:12<28:21,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   1%|▋                                                                         | 6/685 [00:15<28:21,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   1%|▊                                                                         | 7/685 [00:17<28:19,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   1%|▊                                                                         | 8/685 [00:20<28:18,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   1%|▉                                                                         | 9/685 [00:22<28:16,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   1%|█                                                                        | 10/685 [00:25<28:15,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   2%|█▏                                                                       | 11/685 [00:27<28:14,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   2%|█▎                                                                       | 12/685 [00:30<28:12,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   2%|█▍                                                                       | 13/685 [00:32<28:12,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   2%|█▍                                                                       | 14/685 [00:35<28:12,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   2%|█▌                                                                       | 15/685 [00:37<28:12,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   2%|█▋                                                                       | 16/685 [00:40<28:11,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   2%|█▊                                                                       | 17/685 [00:42<28:11,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   3%|█▉                                                                       | 18/685 [00:45<28:10,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   3%|██                                                                       | 19/685 [00:47<28:10,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   3%|██▏                                                                      | 20/685 [00:50<28:10,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   3%|██▏                                                                      | 21/685 [00:52<28:11,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   3%|██▎                                                                      | 22/685 [00:55<28:12,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   3%|██▍                                                                      | 23/685 [00:58<28:12,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   4%|██▌                                                                      | 24/685 [01:00<28:11,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   4%|██▋                                                                      | 25/685 [01:03<28:10,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   4%|██▊                                                                      | 26/685 [01:05<28:10,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   4%|██▉                                                                      | 27/685 [01:08<28:12,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   4%|██▉                                                                      | 28/685 [01:10<28:10,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   4%|███                                                                      | 29/685 [01:13<28:08,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   4%|███▏                                                                     | 30/685 [01:16<28:11,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   5%|███▎                                                                     | 31/685 [01:18<28:13,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   5%|███▍                                                                     | 32/685 [01:21<28:15,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   5%|███▌                                                                     | 33/685 [01:23<28:18,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   5%|███▌                                                                     | 34/685 [01:26<28:20,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   5%|███▋                                                                     | 35/685 [01:29<28:22,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   5%|███▊                                                                     | 36/685 [01:31<28:23,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   5%|███▉                                                                     | 37/685 [01:34<28:24,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   6%|████                                                                     | 38/685 [01:37<28:24,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   6%|████▏                                                                    | 39/685 [01:39<28:25,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   6%|████▎                                                                    | 40/685 [01:42<28:26,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   6%|████▎                                                                    | 41/685 [01:45<28:26,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   6%|████▍                                                                    | 42/685 [01:47<28:25,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   6%|████▌                                                                    | 43/685 [01:50<28:22,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   6%|████▋                                                                    | 44/685 [01:53<28:21,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   7%|████▊                                                                    | 45/685 [01:55<28:20,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   7%|████▉                                                                    | 46/685 [01:58<28:20,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   7%|█████                                                                    | 47/685 [02:01<28:19,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   7%|█████                                                                    | 48/685 [02:03<28:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   7%|█████▏                                                                   | 49/685 [02:06<28:17,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   7%|█████▎                                                                   | 50/685 [02:09<28:15,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:   7%|█████▍                                                                   | 51/685 [02:11<28:14,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   8%|█████▌                                                                   | 52/685 [02:14<28:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   8%|█████▋                                                                   | 53/685 [02:17<28:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   8%|█████▊                                                                   | 54/685 [02:19<28:09,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   8%|█████▊                                                                   | 55/685 [02:22<28:07,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   8%|█████▉                                                                   | 56/685 [02:25<28:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   8%|██████                                                                   | 57/685 [02:27<28:01,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   8%|██████▏                                                                  | 58/685 [02:30<27:57,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   9%|██████▎                                                                  | 59/685 [02:33<27:56,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   9%|██████▍                                                                  | 60/685 [02:35<27:54,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   9%|██████▌                                                                  | 61/685 [02:38<27:51,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   9%|██████▌                                                                  | 62/685 [02:41<27:49,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   9%|██████▋                                                                  | 63/685 [02:43<27:47,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   9%|██████▊                                                                  | 64/685 [02:46<27:45,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:   9%|██████▉                                                                  | 65/685 [02:49<27:42,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  10%|███████                                                                  | 66/685 [02:52<27:42,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  10%|███████▏                                                                 | 67/685 [02:54<27:39,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  10%|███████▏                                                                 | 68/685 [02:57<27:32,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  10%|███████▎                                                                 | 69/685 [03:00<27:28,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  10%|███████▍                                                                 | 70/685 [03:02<27:25,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  10%|███████▌                                                                 | 71/685 [03:05<27:23,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  11%|███████▋                                                                 | 72/685 [03:08<27:22,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  11%|███████▊                                                                 | 73/685 [03:10<27:21,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  11%|███████▉                                                                 | 74/685 [03:13<27:19,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  11%|███████▉                                                                 | 75/685 [03:16<27:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  11%|████████                                                                 | 76/685 [03:18<27:16,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  11%|████████▏                                                                | 77/685 [03:21<27:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  11%|████████▎                                                                | 78/685 [03:24<27:10,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  12%|████████▍                                                                | 79/685 [03:26<27:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  12%|████████▌                                                                | 80/685 [03:29<27:01,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  12%|████████▋                                                                | 81/685 [03:32<26:59,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  12%|████████▋                                                                | 82/685 [03:34<26:56,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  12%|████████▊                                                                | 83/685 [03:37<26:56,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  12%|████████▉                                                                | 84/685 [03:40<26:54,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  12%|█████████                                                                | 85/685 [03:42<26:52,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  13%|█████████▏                                                               | 86/685 [03:45<26:50,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  13%|█████████▎                                                               | 87/685 [03:48<26:46,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  13%|█████████▍                                                               | 88/685 [03:51<26:40,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  13%|█████████▍                                                               | 89/685 [03:53<26:38,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  13%|█████████▌                                                               | 90/685 [03:56<26:34,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  13%|█████████▋                                                               | 91/685 [03:59<26:33,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  13%|█████████▊                                                               | 92/685 [04:01<26:31,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  14%|█████████▉                                                               | 93/685 [04:04<26:27,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  14%|██████████                                                               | 94/685 [04:07<26:25,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  14%|██████████                                                               | 95/685 [04:09<26:23,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  14%|██████████▏                                                              | 96/685 [04:12<26:20,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  14%|██████████▎                                                              | 97/685 [04:15<26:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  14%|██████████▍                                                              | 98/685 [04:17<26:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  14%|██████████▌                                                              | 99/685 [04:20<26:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  15%|██████████▌                                                             | 100/685 [04:23<26:08,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  15%|██████████▌                                                             | 101/685 [04:25<26:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  15%|██████████▋                                                             | 102/685 [04:28<26:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  15%|██████████▊                                                             | 103/685 [04:31<26:00,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  15%|██████████▉                                                             | 104/685 [04:33<25:56,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  15%|███████████                                                             | 105/685 [04:36<25:55,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  15%|███████████▏                                                            | 106/685 [04:39<25:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  16%|███████████▏                                                            | 107/685 [04:41<25:49,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  16%|███████████▎                                                            | 108/685 [04:44<25:47,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  16%|███████████▍                                                            | 109/685 [04:47<25:43,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  16%|███████████▌                                                            | 110/685 [04:50<25:38,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  16%|███████████▋                                                            | 111/685 [04:52<25:35,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  16%|███████████▊                                                            | 112/685 [04:55<25:32,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  16%|███████████▉                                                            | 113/685 [04:58<25:31,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  17%|███████████▉                                                            | 114/685 [05:00<25:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  17%|████████████                                                            | 115/685 [05:03<25:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  17%|████████████▏                                                           | 116/685 [05:06<25:27,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  17%|████████████▎                                                           | 117/685 [05:08<25:26,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  17%|████████████▍                                                           | 118/685 [05:11<25:24,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  17%|████████████▌                                                           | 119/685 [05:14<25:20,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  18%|████████████▌                                                           | 120/685 [05:16<25:15,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  18%|████████████▋                                                           | 121/685 [05:19<25:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  18%|████████████▊                                                           | 122/685 [05:22<25:09,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  18%|████████████▉                                                           | 123/685 [05:24<25:06,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  18%|█████████████                                                           | 124/685 [05:27<25:04,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  18%|█████████████▏                                                          | 125/685 [05:30<25:01,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  18%|█████████████▏                                                          | 126/685 [05:32<25:00,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  19%|█████████████▎                                                          | 127/685 [05:35<24:58,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  19%|█████████████▍                                                          | 128/685 [05:38<24:54,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  19%|█████████████▌                                                          | 129/685 [05:41<24:53,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  19%|█████████████▋                                                          | 130/685 [05:43<24:49,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  19%|█████████████▊                                                          | 131/685 [05:46<24:46,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  19%|█████████████▊                                                          | 132/685 [05:49<24:44,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  19%|█████████████▉                                                          | 133/685 [05:51<24:42,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  20%|██████████████                                                          | 134/685 [05:54<24:39,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  20%|██████████████▏                                                         | 135/685 [05:57<24:34,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  20%|██████████████▎                                                         | 136/685 [05:59<24:32,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  20%|██████████████▍                                                         | 137/685 [06:02<24:30,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  20%|██████████████▌                                                         | 138/685 [06:05<24:27,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  20%|██████████████▌                                                         | 139/685 [06:07<24:25,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  20%|██████████████▋                                                         | 140/685 [06:10<24:21,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  21%|██████████████▊                                                         | 141/685 [06:13<24:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  21%|██████████████▉                                                         | 142/685 [06:15<24:14,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  21%|███████████████                                                         | 143/685 [06:18<24:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  21%|███████████████▏                                                        | 144/685 [06:21<24:08,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  21%|███████████████▏                                                        | 145/685 [06:23<24:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  21%|███████████████▎                                                        | 146/685 [06:26<24:03,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  21%|███████████████▍                                                        | 147/685 [06:29<24:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  22%|███████████████▌                                                        | 148/685 [06:31<23:59,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  22%|███████████████▋                                                        | 149/685 [06:34<23:57,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  22%|███████████████▊                                                        | 150/685 [06:37<23:55,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  22%|███████████████▊                                                        | 151/685 [06:39<23:52,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  22%|███████████████▉                                                        | 152/685 [06:42<23:49,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  22%|████████████████                                                        | 153/685 [06:45<23:44,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  22%|████████████████▏                                                       | 154/685 [06:48<23:40,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  23%|████████████████▎                                                       | 155/685 [06:50<23:38,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  23%|████████████████▍                                                       | 156/685 [06:53<23:37,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  23%|████████████████▌                                                       | 157/685 [06:56<23:34,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  23%|████████████████▌                                                       | 158/685 [06:58<23:32,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  23%|████████████████▋                                                       | 159/685 [07:01<23:28,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  23%|████████████████▊                                                       | 160/685 [07:04<23:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  24%|████████████████▉                                                       | 161/685 [07:06<23:24,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  24%|█████████████████                                                       | 162/685 [07:09<23:19,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  24%|█████████████████▏                                                      | 163/685 [07:12<23:19,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  24%|█████████████████▏                                                      | 164/685 [07:14<23:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  24%|█████████████████▎                                                      | 165/685 [07:17<23:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  24%|█████████████████▍                                                      | 166/685 [07:20<23:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  24%|█████████████████▌                                                      | 167/685 [07:22<23:06,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  25%|█████████████████▋                                                      | 168/685 [07:25<23:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  25%|█████████████████▊                                                      | 169/685 [07:28<23:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  25%|█████████████████▊                                                      | 170/685 [07:30<23:00,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  25%|█████████████████▉                                                      | 171/685 [07:33<22:59,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  25%|██████████████████                                                      | 172/685 [07:36<22:56,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  25%|██████████████████▏                                                     | 173/685 [07:38<22:52,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  25%|██████████████████▎                                                     | 174/685 [07:41<22:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  26%|██████████████████▍                                                     | 175/685 [07:44<22:47,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  26%|██████████████████▍                                                     | 176/685 [07:46<22:42,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  26%|██████████████████▌                                                     | 177/685 [07:49<22:38,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  26%|██████████████████▋                                                     | 178/685 [07:52<22:35,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  26%|██████████████████▊                                                     | 179/685 [07:54<22:33,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  26%|██████████████████▉                                                     | 180/685 [07:57<22:30,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  26%|███████████████████                                                     | 181/685 [08:00<22:27,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  27%|███████████████████▏                                                    | 182/685 [08:03<22:27,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  27%|███████████████████▏                                                    | 183/685 [08:05<22:25,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  27%|███████████████████▎                                                    | 184/685 [08:08<22:24,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  27%|███████████████████▍                                                    | 185/685 [08:11<22:21,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  27%|███████████████████▌                                                    | 186/685 [08:13<22:18,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  27%|███████████████████▋                                                    | 187/685 [08:16<22:16,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  27%|███████████████████▊                                                    | 188/685 [08:19<22:12,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  28%|███████████████████▊                                                    | 189/685 [08:21<22:09,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  28%|███████████████████▉                                                    | 190/685 [08:24<22:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  28%|████████████████████                                                    | 191/685 [08:27<22:00,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  28%|████████████████████▏                                                   | 192/685 [08:29<21:57,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  28%|████████████████████▎                                                   | 193/685 [08:32<21:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  28%|████████████████████▍                                                   | 194/685 [08:35<21:52,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  28%|████████████████████▍                                                   | 195/685 [08:37<21:49,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  29%|████████████████████▌                                                   | 196/685 [08:40<21:46,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  29%|████████████████████▋                                                   | 197/685 [08:43<21:44,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  29%|████████████████████▊                                                   | 198/685 [08:45<21:41,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  29%|████████████████████▉                                                   | 199/685 [08:48<21:39,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  29%|█████████████████████                                                   | 200/685 [08:51<21:40,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  29%|█████████████████████▏                                                  | 201/685 [08:53<21:37,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  29%|█████████████████████▏                                                  | 202/685 [08:56<21:33,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  30%|█████████████████████▎                                                  | 203/685 [08:59<21:30,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  30%|█████████████████████▍                                                  | 204/685 [09:01<21:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  30%|█████████████████████▌                                                  | 205/685 [09:04<21:23,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  30%|█████████████████████▋                                                  | 206/685 [09:07<21:20,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  30%|█████████████████████▊                                                  | 207/685 [09:09<21:17,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  30%|█████████████████████▊                                                  | 208/685 [09:12<21:14,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  31%|█████████████████████▉                                                  | 209/685 [09:15<21:10,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  31%|██████████████████████                                                  | 210/685 [09:17<21:07,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  31%|██████████████████████▏                                                 | 211/685 [09:20<21:05,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  31%|██████████████████████▎                                                 | 212/685 [09:23<21:02,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  31%|██████████████████████▍                                                 | 213/685 [09:25<21:00,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  31%|██████████████████████▍                                                 | 214/685 [09:28<20:58,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  31%|██████████████████████▌                                                 | 215/685 [09:31<20:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  32%|██████████████████████▋                                                 | 216/685 [09:33<20:52,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  32%|██████████████████████▊                                                 | 217/685 [09:36<20:48,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  32%|██████████████████████▉                                                 | 218/685 [09:39<20:46,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  32%|███████████████████████                                                 | 219/685 [09:41<20:44,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  32%|███████████████████████                                                 | 220/685 [09:44<20:42,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  32%|███████████████████████▏                                                | 221/685 [09:47<20:40,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  32%|███████████████████████▎                                                | 222/685 [09:49<20:37,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  33%|███████████████████████▍                                                | 223/685 [09:52<20:35,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  33%|███████████████████████▌                                                | 224/685 [09:55<20:34,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  33%|███████████████████████▋                                                | 225/685 [09:58<20:30,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  33%|███████████████████████▊                                                | 226/685 [10:00<20:28,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  33%|███████████████████████▊                                                | 227/685 [10:03<20:23,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  33%|███████████████████████▉                                                | 228/685 [10:06<20:19,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  33%|████████████████████████                                                | 229/685 [10:08<20:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  34%|████████████████████████▏                                               | 230/685 [10:11<20:14,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  34%|████████████████████████▎                                               | 231/685 [10:14<20:10,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  34%|████████████████████████▍                                               | 232/685 [10:16<20:08,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  34%|████████████████████████▍                                               | 233/685 [10:19<20:05,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  34%|████████████████████████▌                                               | 234/685 [10:22<20:02,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  34%|████████████████████████▋                                               | 235/685 [10:24<20:00,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  34%|████████████████████████▊                                               | 236/685 [10:27<19:57,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  35%|████████████████████████▉                                               | 237/685 [10:30<19:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  35%|█████████████████████████                                               | 238/685 [10:32<19:53,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  35%|█████████████████████████                                               | 239/685 [10:35<19:50,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  35%|█████████████████████████▏                                              | 240/685 [10:38<19:47,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  35%|█████████████████████████▎                                              | 241/685 [10:40<19:44,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  35%|█████████████████████████▍                                              | 242/685 [10:43<19:42,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  35%|█████████████████████████▌                                              | 243/685 [10:46<19:40,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  36%|█████████████████████████▋                                              | 244/685 [10:48<19:39,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  36%|█████████████████████████▊                                              | 245/685 [10:51<19:37,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  36%|█████████████████████████▊                                              | 246/685 [10:54<19:33,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  36%|█████████████████████████▉                                              | 247/685 [10:56<19:31,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  36%|██████████████████████████                                              | 248/685 [10:59<19:28,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  36%|██████████████████████████▏                                             | 249/685 [11:02<19:24,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  36%|██████████████████████████▎                                             | 250/685 [11:04<19:25,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  37%|██████████████████████████▍                                             | 251/685 [11:07<19:19,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  37%|██████████████████████████▍                                             | 252/685 [11:10<19:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  37%|██████████████████████████▌                                             | 253/685 [11:12<19:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  37%|██████████████████████████▋                                             | 254/685 [11:15<19:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  37%|██████████████████████████▊                                             | 255/685 [11:18<19:05,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  37%|██████████████████████████▉                                             | 256/685 [11:20<19:03,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  38%|███████████████████████████                                             | 257/685 [11:23<19:00,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  38%|███████████████████████████                                             | 258/685 [11:26<18:56,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  38%|███████████████████████████▏                                            | 259/685 [11:28<18:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  38%|███████████████████████████▎                                            | 260/685 [11:31<18:53,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  38%|███████████████████████████▍                                            | 261/685 [11:34<18:49,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  38%|███████████████████████████▌                                            | 262/685 [11:36<18:46,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  38%|███████████████████████████▋                                            | 263/685 [11:39<18:43,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  39%|███████████████████████████▋                                            | 264/685 [11:42<18:43,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  39%|███████████████████████████▊                                            | 265/685 [11:44<18:40,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  39%|███████████████████████████▉                                            | 266/685 [11:47<18:37,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  39%|████████████████████████████                                            | 267/685 [11:50<18:34,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  39%|████████████████████████████▏                                           | 268/685 [11:52<18:32,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  39%|████████████████████████████▎                                           | 269/685 [11:55<18:30,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  39%|████████████████████████████▍                                           | 270/685 [11:58<18:28,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  40%|████████████████████████████▍                                           | 271/685 [12:00<18:25,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  40%|████████████████████████████▌                                           | 272/685 [12:03<18:24,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  40%|████████████████████████████▋                                           | 273/685 [12:06<18:20,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  40%|████████████████████████████▊                                           | 274/685 [12:08<18:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  40%|████████████████████████████▉                                           | 275/685 [12:11<18:15,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  40%|█████████████████████████████                                           | 276/685 [12:14<18:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  40%|█████████████████████████████                                           | 277/685 [12:16<18:10,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  41%|█████████████████████████████▏                                          | 278/685 [12:19<18:07,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  41%|█████████████████████████████▎                                          | 279/685 [12:22<18:05,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  41%|█████████████████████████████▍                                          | 280/685 [12:24<18:01,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  41%|█████████████████████████████▌                                          | 281/685 [12:27<17:58,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  41%|█████████████████████████████▋                                          | 282/685 [12:30<17:57,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  41%|█████████████████████████████▋                                          | 283/685 [12:32<17:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  41%|█████████████████████████████▊                                          | 284/685 [12:35<17:51,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  42%|█████████████████████████████▉                                          | 285/685 [12:38<17:48,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  42%|██████████████████████████████                                          | 286/685 [12:40<17:44,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  42%|██████████████████████████████▏                                         | 287/685 [12:43<17:41,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  42%|██████████████████████████████▎                                         | 288/685 [12:46<17:39,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  42%|██████████████████████████████▍                                         | 289/685 [12:48<17:36,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  42%|██████████████████████████████▍                                         | 290/685 [12:51<17:34,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  42%|██████████████████████████████▌                                         | 291/685 [12:54<17:30,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  43%|██████████████████████████████▋                                         | 292/685 [12:56<17:28,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  43%|██████████████████████████████▊                                         | 293/685 [12:59<17:25,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  43%|██████████████████████████████▉                                         | 294/685 [13:02<17:21,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  43%|███████████████████████████████                                         | 295/685 [13:04<17:20,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  43%|███████████████████████████████                                         | 296/685 [13:07<17:17,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  43%|███████████████████████████████▏                                        | 297/685 [13:10<17:15,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  44%|███████████████████████████████▎                                        | 298/685 [13:12<17:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  44%|███████████████████████████████▍                                        | 299/685 [13:15<17:10,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  44%|███████████████████████████████▌                                        | 300/685 [13:18<17:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  44%|███████████████████████████████▋                                        | 301/685 [13:20<17:08,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  44%|███████████████████████████████▋                                        | 302/685 [13:23<17:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  44%|███████████████████████████████▊                                        | 303/685 [13:26<17:01,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  44%|███████████████████████████████▉                                        | 304/685 [13:28<16:59,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  45%|████████████████████████████████                                        | 305/685 [13:31<16:57,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  45%|████████████████████████████████▏                                       | 306/685 [13:34<16:54,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  45%|████████████████████████████████▎                                       | 307/685 [13:36<16:51,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  45%|████████████████████████████████▎                                       | 308/685 [13:39<16:48,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  45%|████████████████████████████████▍                                       | 309/685 [13:42<16:44,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  45%|████████████████████████████████▌                                       | 310/685 [13:44<16:40,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  45%|████████████████████████████████▋                                       | 311/685 [13:47<16:39,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  46%|████████████████████████████████▊                                       | 312/685 [13:50<16:35,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  46%|████████████████████████████████▉                                       | 313/685 [13:52<16:32,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  46%|█████████████████████████████████                                       | 314/685 [13:55<16:30,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  46%|█████████████████████████████████                                       | 315/685 [13:58<16:27,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  46%|█████████████████████████████████▏                                      | 316/685 [14:00<16:24,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  46%|█████████████████████████████████▎                                      | 317/685 [14:03<16:22,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  46%|█████████████████████████████████▍                                      | 318/685 [14:06<16:20,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  47%|█████████████████████████████████▌                                      | 319/685 [14:08<16:17,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  47%|█████████████████████████████████▋                                      | 320/685 [14:11<16:16,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  47%|█████████████████████████████████▋                                      | 321/685 [14:14<16:15,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  47%|█████████████████████████████████▊                                      | 322/685 [14:17<16:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  47%|█████████████████████████████████▉                                      | 323/685 [14:19<16:08,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  47%|██████████████████████████████████                                      | 324/685 [14:22<16:05,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  47%|██████████████████████████████████▏                                     | 325/685 [14:25<16:02,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  48%|██████████████████████████████████▎                                     | 326/685 [14:27<15:58,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  48%|██████████████████████████████████▎                                     | 327/685 [14:30<15:54,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  48%|██████████████████████████████████▍                                     | 328/685 [14:33<15:53,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  48%|██████████████████████████████████▌                                     | 329/685 [14:35<15:50,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  48%|██████████████████████████████████▋                                     | 330/685 [14:38<15:47,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  48%|██████████████████████████████████▊                                     | 331/685 [14:41<15:45,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  48%|██████████████████████████████████▉                                     | 332/685 [14:43<15:42,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  49%|███████████████████████████████████                                     | 333/685 [14:46<15:38,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  49%|███████████████████████████████████                                     | 334/685 [14:49<15:36,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  49%|███████████████████████████████████▏                                    | 335/685 [14:51<15:34,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  49%|███████████████████████████████████▎                                    | 336/685 [14:54<15:31,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  49%|███████████████████████████████████▍                                    | 337/685 [14:57<15:28,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  49%|███████████████████████████████████▌                                    | 338/685 [14:59<15:26,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  49%|███████████████████████████████████▋                                    | 339/685 [15:02<15:23,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  50%|███████████████████████████████████▋                                    | 340/685 [15:05<15:20,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  50%|███████████████████████████████████▊                                    | 341/685 [15:07<15:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  50%|███████████████████████████████████▉                                    | 342/685 [15:10<15:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  50%|████████████████████████████████████                                    | 343/685 [15:13<15:14,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  50%|████████████████████████████████████▏                                   | 344/685 [15:15<15:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  50%|████████████████████████████████████▎                                   | 345/685 [15:18<15:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  51%|████████████████████████████████████▎                                   | 346/685 [15:21<15:07,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  51%|████████████████████████████████████▍                                   | 347/685 [15:23<15:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  51%|████████████████████████████████████▌                                   | 348/685 [15:26<15:01,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  51%|████████████████████████████████████▋                                   | 349/685 [15:29<14:57,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  51%|████████████████████████████████████▊                                   | 350/685 [15:31<14:58,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  51%|████████████████████████████████████▉                                   | 351/685 [15:34<14:55,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  51%|████████████████████████████████████▉                                   | 352/685 [15:37<14:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  52%|█████████████████████████████████████                                   | 353/685 [15:39<14:47,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  52%|█████████████████████████████████████▏                                  | 354/685 [15:42<14:45,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  52%|█████████████████████████████████████▎                                  | 355/685 [15:45<14:41,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  52%|█████████████████████████████████████▍                                  | 356/685 [15:47<14:39,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  52%|█████████████████████████████████████▌                                  | 357/685 [15:50<14:36,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  52%|█████████████████████████████████████▋                                  | 358/685 [15:53<14:33,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  52%|█████████████████████████████████████▋                                  | 359/685 [15:55<14:30,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  53%|█████████████████████████████████████▊                                  | 360/685 [15:58<14:27,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  53%|█████████████████████████████████████▉                                  | 361/685 [16:01<14:23,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  53%|██████████████████████████████████████                                  | 362/685 [16:03<14:20,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  53%|██████████████████████████████████████▏                                 | 363/685 [16:06<14:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  53%|██████████████████████████████████████▎                                 | 364/685 [16:09<14:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  53%|██████████████████████████████████████▎                                 | 365/685 [16:11<14:13,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  53%|██████████████████████████████████████▍                                 | 366/685 [16:14<14:10,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  54%|██████████████████████████████████████▌                                 | 367/685 [16:17<14:08,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  54%|██████████████████████████████████████▋                                 | 368/685 [16:19<14:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  54%|██████████████████████████████████████▊                                 | 369/685 [16:22<14:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  54%|██████████████████████████████████████▉                                 | 370/685 [16:25<14:01,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  54%|██████████████████████████████████████▉                                 | 371/685 [16:27<13:58,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  54%|███████████████████████████████████████                                 | 372/685 [16:30<13:56,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  54%|███████████████████████████████████████▏                                | 373/685 [16:33<13:53,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  55%|███████████████████████████████████████▎                                | 374/685 [16:35<13:51,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  55%|███████████████████████████████████████▍                                | 375/685 [16:38<13:48,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  55%|███████████████████████████████████████▌                                | 376/685 [16:41<13:45,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  55%|███████████████████████████████████████▋                                | 377/685 [16:43<13:41,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  55%|███████████████████████████████████████▋                                | 378/685 [16:46<13:38,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  55%|███████████████████████████████████████▊                                | 379/685 [16:49<13:35,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  55%|███████████████████████████████████████▉                                | 380/685 [16:51<13:33,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  56%|████████████████████████████████████████                                | 381/685 [16:54<13:30,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  56%|████████████████████████████████████████▏                               | 382/685 [16:57<13:28,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  56%|████████████████████████████████████████▎                               | 383/685 [16:59<13:25,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  56%|████████████████████████████████████████▎                               | 384/685 [17:02<13:23,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  56%|████████████████████████████████████████▍                               | 385/685 [17:05<13:20,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  56%|████████████████████████████████████████▌                               | 386/685 [17:07<13:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  56%|████████████████████████████████████████▋                               | 387/685 [17:10<13:14,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  57%|████████████████████████████████████████▊                               | 388/685 [17:13<13:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  57%|████████████████████████████████████████▉                               | 389/685 [17:15<13:10,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  57%|████████████████████████████████████████▉                               | 390/685 [17:18<13:07,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  57%|█████████████████████████████████████████                               | 391/685 [17:21<13:05,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  57%|█████████████████████████████████████████▏                              | 392/685 [17:23<13:03,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  57%|█████████████████████████████████████████▎                              | 393/685 [17:26<13:01,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  58%|█████████████████████████████████████████▍                              | 394/685 [17:29<12:58,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  58%|█████████████████████████████████████████▌                              | 395/685 [17:32<12:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  58%|█████████████████████████████████████████▌                              | 396/685 [17:34<12:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  58%|█████████████████████████████████████████▋                              | 397/685 [17:37<12:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  58%|█████████████████████████████████████████▊                              | 398/685 [17:40<12:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  58%|█████████████████████████████████████████▉                              | 399/685 [17:42<12:45,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  58%|██████████████████████████████████████████                              | 400/685 [17:45<12:45,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  59%|██████████████████████████████████████████▏                             | 401/685 [17:48<12:41,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  59%|██████████████████████████████████████████▎                             | 402/685 [17:50<12:37,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  59%|██████████████████████████████████████████▎                             | 403/685 [17:53<12:35,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  59%|██████████████████████████████████████████▍                             | 404/685 [17:56<12:32,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  59%|██████████████████████████████████████████▌                             | 405/685 [17:58<12:30,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  59%|██████████████████████████████████████████▋                             | 406/685 [18:01<12:27,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  59%|██████████████████████████████████████████▊                             | 407/685 [18:04<12:24,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  60%|██████████████████████████████████████████▉                             | 408/685 [18:06<12:22,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  60%|██████████████████████████████████████████▉                             | 409/685 [18:09<12:19,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  60%|███████████████████████████████████████████                             | 410/685 [18:12<12:16,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  60%|███████████████████████████████████████████▏                            | 411/685 [18:14<12:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  60%|███████████████████████████████████████████▎                            | 412/685 [18:17<12:10,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  60%|███████████████████████████████████████████▍                            | 413/685 [18:20<12:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  60%|███████████████████████████████████████████▌                            | 414/685 [18:22<12:03,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  61%|███████████████████████████████████████████▌                            | 415/685 [18:25<12:01,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  61%|███████████████████████████████████████████▋                            | 416/685 [18:28<11:59,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  61%|███████████████████████████████████████████▊                            | 417/685 [18:30<11:58,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  61%|███████████████████████████████████████████▉                            | 418/685 [18:33<11:55,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  61%|████████████████████████████████████████████                            | 419/685 [18:36<11:52,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  61%|████████████████████████████████████████████▏                           | 420/685 [18:38<11:49,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  61%|████████████████████████████████████████████▎                           | 421/685 [18:41<11:46,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  62%|████████████████████████████████████████████▎                           | 422/685 [18:44<11:44,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  62%|████████████████████████████████████████████▍                           | 423/685 [18:46<11:41,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  62%|████████████████████████████████████████████▌                           | 424/685 [18:49<11:37,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  62%|████████████████████████████████████████████▋                           | 425/685 [18:52<11:34,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  62%|████████████████████████████████████████████▊                           | 426/685 [18:54<11:31,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  62%|████████████████████████████████████████████▉                           | 427/685 [18:57<11:29,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  62%|████████████████████████████████████████████▉                           | 428/685 [19:00<11:26,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  63%|█████████████████████████████████████████████                           | 429/685 [19:03<11:23,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  63%|█████████████████████████████████████████████▏                          | 430/685 [19:05<11:20,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  63%|█████████████████████████████████████████████▎                          | 431/685 [19:08<11:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  63%|█████████████████████████████████████████████▍                          | 432/685 [19:11<11:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  63%|█████████████████████████████████████████████▌                          | 433/685 [19:13<11:13,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  63%|█████████████████████████████████████████████▌                          | 434/685 [19:16<11:10,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  64%|█████████████████████████████████████████████▋                          | 435/685 [19:19<11:08,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  64%|█████████████████████████████████████████████▊                          | 436/685 [19:21<11:05,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  64%|█████████████████████████████████████████████▉                          | 437/685 [19:24<11:03,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  64%|██████████████████████████████████████████████                          | 438/685 [19:27<11:00,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  64%|██████████████████████████████████████████████▏                         | 439/685 [19:29<10:58,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  64%|██████████████████████████████████████████████▏                         | 440/685 [19:32<10:56,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  64%|██████████████████████████████████████████████▎                         | 441/685 [19:35<10:54,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  65%|██████████████████████████████████████████████▍                         | 442/685 [19:37<10:52,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  65%|██████████████████████████████████████████████▌                         | 443/685 [19:40<10:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  65%|██████████████████████████████████████████████▋                         | 444/685 [19:43<10:45,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  65%|██████████████████████████████████████████████▊                         | 445/685 [19:45<10:42,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  65%|██████████████████████████████████████████████▉                         | 446/685 [19:48<10:39,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  65%|██████████████████████████████████████████████▉                         | 447/685 [19:51<10:37,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  65%|███████████████████████████████████████████████                         | 448/685 [19:53<10:34,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  66%|███████████████████████████████████████████████▏                        | 449/685 [19:56<10:32,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  66%|███████████████████████████████████████████████▎                        | 450/685 [19:59<10:33,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  66%|███████████████████████████████████████████████▍                        | 451/685 [20:01<10:30,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  66%|███████████████████████████████████████████████▌                        | 452/685 [20:04<10:25,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  66%|███████████████████████████████████████████████▌                        | 453/685 [20:07<10:22,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  66%|███████████████████████████████████████████████▋                        | 454/685 [20:09<10:18,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  66%|███████████████████████████████████████████████▊                        | 455/685 [20:12<10:16,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  67%|███████████████████████████████████████████████▉                        | 456/685 [20:15<10:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  67%|████████████████████████████████████████████████                        | 457/685 [20:18<10:10,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  67%|████████████████████████████████████████████████▏                       | 458/685 [20:20<10:08,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  67%|████████████████████████████████████████████████▏                       | 459/685 [20:23<10:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  67%|████████████████████████████████████████████████▎                       | 460/685 [20:26<10:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  67%|████████████████████████████████████████████████▍                       | 461/685 [20:28<09:59,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  67%|████████████████████████████████████████████████▌                       | 462/685 [20:31<09:57,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  68%|████████████████████████████████████████████████▋                       | 463/685 [20:34<09:54,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  68%|████████████████████████████████████████████████▊                       | 464/685 [20:36<09:52,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  68%|████████████████████████████████████████████████▉                       | 465/685 [20:39<09:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  68%|████████████████████████████████████████████████▉                       | 466/685 [20:42<09:46,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  68%|█████████████████████████████████████████████████                       | 467/685 [20:44<09:43,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  68%|█████████████████████████████████████████████████▏                      | 468/685 [20:47<09:40,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  68%|█████████████████████████████████████████████████▎                      | 469/685 [20:50<09:37,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  69%|█████████████████████████████████████████████████▍                      | 470/685 [20:52<09:35,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  69%|█████████████████████████████████████████████████▌                      | 471/685 [20:55<09:32,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  69%|█████████████████████████████████████████████████▌                      | 472/685 [20:58<09:30,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  69%|█████████████████████████████████████████████████▋                      | 473/685 [21:00<09:27,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  69%|█████████████████████████████████████████████████▊                      | 474/685 [21:03<09:24,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  69%|█████████████████████████████████████████████████▉                      | 475/685 [21:06<09:21,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  69%|██████████████████████████████████████████████████                      | 476/685 [21:08<09:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  70%|██████████████████████████████████████████████████▏                     | 477/685 [21:11<09:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  70%|██████████████████████████████████████████████████▏                     | 478/685 [21:14<09:14,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  70%|██████████████████████████████████████████████████▎                     | 479/685 [21:16<09:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  70%|██████████████████████████████████████████████████▍                     | 480/685 [21:19<09:09,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  70%|██████████████████████████████████████████████████▌                     | 481/685 [21:22<09:06,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  70%|██████████████████████████████████████████████████▋                     | 482/685 [21:24<09:03,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  71%|██████████████████████████████████████████████████▊                     | 483/685 [21:27<08:59,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  71%|██████████████████████████████████████████████████▊                     | 484/685 [21:30<08:56,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  71%|██████████████████████████████████████████████████▉                     | 485/685 [21:32<08:53,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  71%|███████████████████████████████████████████████████                     | 486/685 [21:35<08:51,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  71%|███████████████████████████████████████████████████▏                    | 487/685 [21:38<08:49,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  71%|███████████████████████████████████████████████████▎                    | 488/685 [21:40<08:47,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  71%|███████████████████████████████████████████████████▍                    | 489/685 [21:43<08:44,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  72%|███████████████████████████████████████████████████▌                    | 490/685 [21:46<08:42,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  72%|███████████████████████████████████████████████████▌                    | 491/685 [21:48<08:39,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  72%|███████████████████████████████████████████████████▋                    | 492/685 [21:51<08:36,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  72%|███████████████████████████████████████████████████▊                    | 493/685 [21:54<08:34,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  72%|███████████████████████████████████████████████████▉                    | 494/685 [21:57<08:31,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  72%|████████████████████████████████████████████████████                    | 495/685 [21:59<08:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  72%|████████████████████████████████████████████████████▏                   | 496/685 [22:02<08:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  73%|████████████████████████████████████████████████████▏                   | 497/685 [22:05<08:23,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  73%|████████████████████████████████████████████████████▎                   | 498/685 [22:07<08:21,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  73%|████████████████████████████████████████████████████▍                   | 499/685 [22:10<08:18,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  73%|████████████████████████████████████████████████████▌                   | 500/685 [22:13<08:18,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  73%|████████████████████████████████████████████████████▋                   | 501/685 [22:15<08:14,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  73%|████████████████████████████████████████████████████▊                   | 502/685 [22:18<08:12,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  73%|████████████████████████████████████████████████████▊                   | 503/685 [22:21<08:09,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  74%|████████████████████████████████████████████████████▉                   | 504/685 [22:23<08:06,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  74%|█████████████████████████████████████████████████████                   | 505/685 [22:26<08:03,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  74%|█████████████████████████████████████████████████████▏                  | 506/685 [22:29<07:59,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  74%|█████████████████████████████████████████████████████▎                  | 507/685 [22:31<07:56,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  74%|█████████████████████████████████████████████████████▍                  | 508/685 [22:34<07:54,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  74%|█████████████████████████████████████████████████████▌                  | 509/685 [22:37<07:51,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  74%|█████████████████████████████████████████████████████▌                  | 510/685 [22:39<07:47,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  75%|█████████████████████████████████████████████████████▋                  | 511/685 [22:42<07:45,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  75%|█████████████████████████████████████████████████████▊                  | 512/685 [22:45<07:42,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  75%|█████████████████████████████████████████████████████▉                  | 513/685 [22:47<07:40,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  75%|██████████████████████████████████████████████████████                  | 514/685 [22:50<07:37,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  75%|██████████████████████████████████████████████████████▏                 | 515/685 [22:53<07:34,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  75%|██████████████████████████████████████████████████████▏                 | 516/685 [22:55<07:31,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  75%|██████████████████████████████████████████████████████▎                 | 517/685 [22:58<07:28,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  76%|██████████████████████████████████████████████████████▍                 | 518/685 [23:01<07:25,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  76%|██████████████████████████████████████████████████████▌                 | 519/685 [23:03<07:23,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  76%|██████████████████████████████████████████████████████▋                 | 520/685 [23:06<07:20,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  76%|██████████████████████████████████████████████████████▊                 | 521/685 [23:09<07:17,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  76%|██████████████████████████████████████████████████████▊                 | 522/685 [23:11<07:15,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  76%|██████████████████████████████████████████████████████▉                 | 523/685 [23:14<07:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  76%|███████████████████████████████████████████████████████                 | 524/685 [23:17<07:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  77%|███████████████████████████████████████████████████████▏                | 525/685 [23:20<07:07,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  77%|███████████████████████████████████████████████████████▎                | 526/685 [23:22<07:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  77%|███████████████████████████████████████████████████████▍                | 527/685 [23:25<07:02,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  77%|███████████████████████████████████████████████████████▍                | 528/685 [23:28<06:59,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  77%|███████████████████████████████████████████████████████▌                | 529/685 [23:30<06:57,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  77%|███████████████████████████████████████████████████████▋                | 530/685 [23:33<06:54,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  78%|███████████████████████████████████████████████████████▊                | 531/685 [23:36<06:51,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  78%|███████████████████████████████████████████████████████▉                | 532/685 [23:38<06:47,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  78%|████████████████████████████████████████████████████████                | 533/685 [23:41<06:44,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  78%|████████████████████████████████████████████████████████▏               | 534/685 [23:44<06:42,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  78%|████████████████████████████████████████████████████████▏               | 535/685 [23:46<06:40,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  78%|████████████████████████████████████████████████████████▎               | 536/685 [23:49<06:38,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  78%|████████████████████████████████████████████████████████▍               | 537/685 [23:52<06:35,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  79%|████████████████████████████████████████████████████████▌               | 538/685 [23:54<06:33,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  79%|████████████████████████████████████████████████████████▋               | 539/685 [23:57<06:30,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  79%|████████████████████████████████████████████████████████▊               | 540/685 [24:00<06:28,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  79%|████████████████████████████████████████████████████████▊               | 541/685 [24:02<06:25,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  79%|████████████████████████████████████████████████████████▉               | 542/685 [24:05<06:22,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  79%|█████████████████████████████████████████████████████████               | 543/685 [24:08<06:20,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  79%|█████████████████████████████████████████████████████████▏              | 544/685 [24:10<06:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  80%|█████████████████████████████████████████████████████████▎              | 545/685 [24:13<06:14,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  80%|█████████████████████████████████████████████████████████▍              | 546/685 [24:16<06:11,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  80%|█████████████████████████████████████████████████████████▍              | 547/685 [24:18<06:08,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  80%|█████████████████████████████████████████████████████████▌              | 548/685 [24:21<06:05,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  80%|█████████████████████████████████████████████████████████▋              | 549/685 [24:24<06:03,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  80%|█████████████████████████████████████████████████████████▊              | 550/685 [24:26<06:03,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  80%|█████████████████████████████████████████████████████████▉              | 551/685 [24:29<05:59,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  81%|██████████████████████████████████████████████████████████              | 552/685 [24:32<05:56,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  81%|██████████████████████████████████████████████████████████▏             | 553/685 [24:34<05:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  81%|██████████████████████████████████████████████████████████▏             | 554/685 [24:37<05:50,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  81%|██████████████████████████████████████████████████████████▎             | 555/685 [24:40<05:47,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  81%|██████████████████████████████████████████████████████████▍             | 556/685 [24:42<05:45,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  81%|██████████████████████████████████████████████████████████▌             | 557/685 [24:45<05:42,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  81%|██████████████████████████████████████████████████████████▋             | 558/685 [24:48<05:39,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  82%|██████████████████████████████████████████████████████████▊             | 559/685 [24:50<05:37,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  82%|██████████████████████████████████████████████████████████▊             | 560/685 [24:53<05:34,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  82%|██████████████████████████████████████████████████████████▉             | 561/685 [24:56<05:31,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  82%|███████████████████████████████████████████████████████████             | 562/685 [24:58<05:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  82%|███████████████████████████████████████████████████████████▏            | 563/685 [25:01<05:26,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  82%|███████████████████████████████████████████████████████████▎            | 564/685 [25:04<05:23,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  82%|███████████████████████████████████████████████████████████▍            | 565/685 [25:06<05:20,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  83%|███████████████████████████████████████████████████████████▍            | 566/685 [25:09<05:17,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  83%|███████████████████████████████████████████████████████████▌            | 567/685 [25:12<05:15,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  83%|███████████████████████████████████████████████████████████▋            | 568/685 [25:15<05:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  83%|███████████████████████████████████████████████████████████▊            | 569/685 [25:17<05:10,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  83%|███████████████████████████████████████████████████████████▉            | 570/685 [25:20<05:07,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  83%|████████████████████████████████████████████████████████████            | 571/685 [25:23<05:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  84%|████████████████████████████████████████████████████████████            | 572/685 [25:25<05:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  84%|████████████████████████████████████████████████████████████▏           | 573/685 [25:28<05:00,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  84%|████████████████████████████████████████████████████████████▎           | 574/685 [25:31<04:57,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  84%|████████████████████████████████████████████████████████████▍           | 575/685 [25:33<04:54,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  84%|████████████████████████████████████████████████████████████▌           | 576/685 [25:36<04:51,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  84%|████████████████████████████████████████████████████████████▋           | 577/685 [25:39<04:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  84%|████████████████████████████████████████████████████████████▊           | 578/685 [25:41<04:45,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  85%|████████████████████████████████████████████████████████████▊           | 579/685 [25:44<04:43,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  85%|████████████████████████████████████████████████████████████▉           | 580/685 [25:47<04:40,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  85%|█████████████████████████████████████████████████████████████           | 581/685 [25:49<04:38,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  85%|█████████████████████████████████████████████████████████████▏          | 582/685 [25:52<04:35,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  85%|█████████████████████████████████████████████████████████████▎          | 583/685 [25:55<04:32,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  85%|█████████████████████████████████████████████████████████████▍          | 584/685 [25:57<04:30,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  85%|█████████████████████████████████████████████████████████████▍          | 585/685 [26:00<04:28,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  86%|█████████████████████████████████████████████████████████████▌          | 586/685 [26:03<04:25,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  86%|█████████████████████████████████████████████████████████████▋          | 587/685 [26:05<04:22,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  86%|█████████████████████████████████████████████████████████████▊          | 588/685 [26:08<04:19,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  86%|█████████████████████████████████████████████████████████████▉          | 589/685 [26:11<04:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  86%|██████████████████████████████████████████████████████████████          | 590/685 [26:13<04:14,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  86%|██████████████████████████████████████████████████████████████          | 591/685 [26:16<04:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  86%|██████████████████████████████████████████████████████████████▏         | 592/685 [26:19<04:08,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  87%|██████████████████████████████████████████████████████████████▎         | 593/685 [26:21<04:05,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  87%|██████████████████████████████████████████████████████████████▍         | 594/685 [26:24<04:03,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  87%|██████████████████████████████████████████████████████████████▌         | 595/685 [26:27<04:00,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  87%|██████████████████████████████████████████████████████████████▋         | 596/685 [26:29<03:57,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  87%|██████████████████████████████████████████████████████████████▊         | 597/685 [26:32<03:54,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  87%|██████████████████████████████████████████████████████████████▊         | 598/685 [26:35<03:52,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  87%|██████████████████████████████████████████████████████████████▉         | 599/685 [26:37<03:49,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  88%|███████████████████████████████████████████████████████████████         | 600/685 [26:40<03:48,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  88%|███████████████████████████████████████████████████████████████▏        | 601/685 [26:43<03:45,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  88%|███████████████████████████████████████████████████████████████▎        | 602/685 [26:46<03:43,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  88%|███████████████████████████████████████████████████████████████▍        | 603/685 [26:48<03:40,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  88%|███████████████████████████████████████████████████████████████▍        | 604/685 [26:51<03:37,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  88%|███████████████████████████████████████████████████████████████▌        | 605/685 [26:54<03:34,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  88%|███████████████████████████████████████████████████████████████▋        | 606/685 [26:56<03:31,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  89%|███████████████████████████████████████████████████████████████▊        | 607/685 [26:59<03:28,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  89%|███████████████████████████████████████████████████████████████▉        | 608/685 [27:02<03:25,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  89%|████████████████████████████████████████████████████████████████        | 609/685 [27:04<03:23,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  89%|████████████████████████████████████████████████████████████████        | 610/685 [27:07<03:20,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  89%|████████████████████████████████████████████████████████████████▏       | 611/685 [27:10<03:18,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  89%|████████████████████████████████████████████████████████████████▎       | 612/685 [27:12<03:15,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  89%|████████████████████████████████████████████████████████████████▍       | 613/685 [27:15<03:12,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  90%|████████████████████████████████████████████████████████████████▌       | 614/685 [27:18<03:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  90%|████████████████████████████████████████████████████████████████▋       | 615/685 [27:20<03:07,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  90%|████████████████████████████████████████████████████████████████▋       | 616/685 [27:23<03:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  90%|████████████████████████████████████████████████████████████████▊       | 617/685 [27:26<03:01,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  90%|████████████████████████████████████████████████████████████████▉       | 618/685 [27:28<02:59,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  90%|█████████████████████████████████████████████████████████████████       | 619/685 [27:31<02:56,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  91%|█████████████████████████████████████████████████████████████████▏      | 620/685 [27:34<02:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  91%|█████████████████████████████████████████████████████████████████▎      | 621/685 [27:36<02:51,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  91%|█████████████████████████████████████████████████████████████████▍      | 622/685 [27:39<02:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  91%|█████████████████████████████████████████████████████████████████▍      | 623/685 [27:42<02:45,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  91%|█████████████████████████████████████████████████████████████████▌      | 624/685 [27:44<02:43,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  91%|█████████████████████████████████████████████████████████████████▋      | 625/685 [27:47<02:40,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  91%|█████████████████████████████████████████████████████████████████▊      | 626/685 [27:50<02:37,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  92%|█████████████████████████████████████████████████████████████████▉      | 627/685 [27:52<02:35,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  92%|██████████████████████████████████████████████████████████████████      | 628/685 [27:55<02:32,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  92%|██████████████████████████████████████████████████████████████████      | 629/685 [27:58<02:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  92%|██████████████████████████████████████████████████████████████████▏     | 630/685 [28:00<02:27,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  92%|██████████████████████████████████████████████████████████████████▎     | 631/685 [28:03<02:24,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  92%|██████████████████████████████████████████████████████████████████▍     | 632/685 [28:06<02:22,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  92%|██████████████████████████████████████████████████████████████████▌     | 633/685 [28:09<02:19,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  93%|██████████████████████████████████████████████████████████████████▋     | 634/685 [28:11<02:16,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  93%|██████████████████████████████████████████████████████████████████▋     | 635/685 [28:14<02:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  93%|██████████████████████████████████████████████████████████████████▊     | 636/685 [28:17<02:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  93%|██████████████████████████████████████████████████████████████████▉     | 637/685 [28:19<02:08,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  93%|███████████████████████████████████████████████████████████████████     | 638/685 [28:22<02:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  93%|███████████████████████████████████████████████████████████████████▏    | 639/685 [28:25<02:03,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  93%|███████████████████████████████████████████████████████████████████▎    | 640/685 [28:27<02:00,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  94%|███████████████████████████████████████████████████████████████████▍    | 641/685 [28:30<01:58,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  94%|███████████████████████████████████████████████████████████████████▍    | 642/685 [28:33<01:55,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  94%|███████████████████████████████████████████████████████████████████▌    | 643/685 [28:35<01:52,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  94%|███████████████████████████████████████████████████████████████████▋    | 644/685 [28:38<01:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  94%|███████████████████████████████████████████████████████████████████▊    | 645/685 [28:41<01:47,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  94%|███████████████████████████████████████████████████████████████████▉    | 646/685 [28:43<01:44,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  94%|████████████████████████████████████████████████████████████████████    | 647/685 [28:46<01:41,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  95%|████████████████████████████████████████████████████████████████████    | 648/685 [28:49<01:39,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  95%|████████████████████████████████████████████████████████████████████▏   | 649/685 [28:51<01:36,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  95%|████████████████████████████████████████████████████████████████████▎   | 650/685 [28:54<01:34,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  95%|████████████████████████████████████████████████████████████████████▍   | 651/685 [28:57<01:31,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  95%|████████████████████████████████████████████████████████████████████▌   | 652/685 [29:00<01:28,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  95%|████████████████████████████████████████████████████████████████████▋   | 653/685 [29:02<01:26,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  95%|████████████████████████████████████████████████████████████████████▋   | 654/685 [29:05<01:23,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  96%|████████████████████████████████████████████████████████████████████▊   | 655/685 [29:08<01:20,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  96%|████████████████████████████████████████████████████████████████████▉   | 656/685 [29:10<01:17,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  96%|█████████████████████████████████████████████████████████████████████   | 657/685 [29:13<01:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  96%|█████████████████████████████████████████████████████████████████████▏  | 658/685 [29:16<01:12,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  96%|█████████████████████████████████████████████████████████████████████▎  | 659/685 [29:18<01:09,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  96%|█████████████████████████████████████████████████████████████████████▎  | 660/685 [29:21<01:07,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  96%|█████████████████████████████████████████████████████████████████████▍  | 661/685 [29:24<01:04,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  97%|█████████████████████████████████████████████████████████████████████▌  | 662/685 [29:26<01:01,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  97%|█████████████████████████████████████████████████████████████████████▋  | 663/685 [29:29<00:58,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  97%|█████████████████████████████████████████████████████████████████████▊  | 664/685 [29:32<00:56,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  97%|█████████████████████████████████████████████████████████████████████▉  | 665/685 [29:34<00:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  97%|██████████████████████████████████████████████████████████████████████  | 666/685 [29:37<00:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  97%|██████████████████████████████████████████████████████████████████████  | 667/685 [29:40<00:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  98%|██████████████████████████████████████████████████████████████████████▏ | 668/685 [29:42<00:45,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  98%|██████████████████████████████████████████████████████████████████████▎ | 669/685 [29:45<00:42,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  98%|██████████████████████████████████████████████████████████████████████▍ | 670/685 [29:48<00:40,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  98%|██████████████████████████████████████████████████████████████████████▌ | 671/685 [29:50<00:37,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  98%|██████████████████████████████████████████████████████████████████████▋ | 672/685 [29:53<00:34,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  98%|██████████████████████████████████████████████████████████████████████▋ | 673/685 [29:56<00:32,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  98%|██████████████████████████████████████████████████████████████████████▊ | 674/685 [29:59<00:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  99%|██████████████████████████████████████████████████████████████████████▉ | 675/685 [30:01<00:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  99%|███████████████████████████████████████████████████████████████████████ | 676/685 [30:04<00:24,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  99%|███████████████████████████████████████████████████████████████████████▏| 677/685 [30:07<00:21,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  99%|███████████████████████████████████████████████████████████████████████▎| 678/685 [30:09<00:18,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  99%|███████████████████████████████████████████████████████████████████████▎| 679/685 [30:12<00:16,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  99%|███████████████████████████████████████████████████████████████████████▍| 680/685 [30:15<00:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries:  99%|███████████████████████████████████████████████████████████████████████▌| 681/685 [30:17<00:10,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries: 100%|███████████████████████████████████████████████████████████████████████▋| 682/685 [30:20<00:08,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries: 100%|███████████████████████████████████████████████████████████████████████▊| 683/685 [30:23<00:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries: 100%|███████████████████████████████████████████████████████████████████████▉| 684/685 [30:25<00:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Summaries: 100%|████████████████████████████████████████████████████████████████████████| 685/685 [30:27<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving at batch 685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_path = \"./gpt2-docstring-model\"  # ✅ just the path, not the model itself\n",
    "\n",
    "chosen_config = {\n",
    "    \"max_new_tokens\": 64,\n",
    "    \"num_beams\": 4,\n",
    "    \"early_stopping\": True,\n",
    "    \"repetition_penalty\": 1.3\n",
    "}\n",
    "save_path = \"codet5_test_sampling_output_beam_repeat.csv\"\n",
    "# .to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = CodeSummaryGenerator(model_path, chosen_config, device)\n",
    "test_tokenized, references = generator.preprocess_dataset(\"test\")\n",
    "generator.generate_summaries(test_tokenized, references, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debf0ea-2888-4df5-8f07-b0f8e2849e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Load predictions CSV\n",
    "df = pd.read_csv(\"codet5_val_predictions_cleaned_2.csv\")\n",
    "\n",
    "# Extract predictions and references\n",
    "predictions = df[\"predicted_summary\"].astype(str).tolist()\n",
    "references = df[\"gold_summary\"].astype(str).tolist()\n",
    "\n",
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# ROUGE\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "print(\"📊 ROUGE Scores:\")\n",
    "for k, v in rouge_scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# BLEU\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"\\n📘 BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "\n",
    "# BERTScore\n",
    "bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"en\", device=\"cuda\")\n",
    "print(f\"\\n🧠 BERTScore F1 (avg): {np.mean(bert_score['f1']):.4f}\")\n",
    "\n",
    "# Function to compute average token repetition per summary\n",
    "def avg_token_repetition(predictions):\n",
    "    rep_counts = []\n",
    "    for text in predictions:\n",
    "        tokens = text.strip().split()\n",
    "        counts = Counter(tokens)\n",
    "        repeated_tokens = sum(v for v in counts.values() if v > 1)\n",
    "        rep_counts.append(repeated_tokens / max(1, len(tokens)))\n",
    "    return np.mean(rep_counts)\n",
    "repetition = avg_token_repetition(predictions)\n",
    "\n",
    "\n",
    "# Exact Match\n",
    "exact_matches = sum(p.strip() == r.strip() for p, r in zip(predictions, references))\n",
    "exact_match_accuracy = exact_matches / len(references)\n",
    "print(f\"\\n✅ Exact Match Accuracy: {exact_match_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
